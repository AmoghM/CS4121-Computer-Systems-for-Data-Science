{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import re,sys\n",
    "from operator import add\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql import SQLContext as sqlcontext\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "class PageRank:\n",
    "    def __init__(self,path,max_iter=10,init_rank=1.0):\n",
    "        self.path = path\n",
    "        self.max_iter = max_iter\n",
    "        self.init_rank = init_rank\n",
    "\n",
    "    def fit(self):\n",
    "        spark = SparkSession.builder.appName(\"PageRank\").getOrCreate()\n",
    "        data = spark.read.text(self.path).rdd.map(lambda row: row[0])\n",
    "#         data = spark.read.format('csv').options(delimiter='\\t').load(self.path).rdd.map(lambda row:row[0])\n",
    "        '''\n",
    "        OUTPUT: data.collect() will retrieve the content inside the RDD.\n",
    "        ['article1\\tarticle2','article1\\tarticle4','article2\\tarticle3','article3\\tarticle1','article4\\tarticle2','article5\\tarticle6']\n",
    "        '''\n",
    "        \n",
    "        adj_list = data.map(lambda data: self.create_adj_list(data)).distinct().groupByKey().cache()\n",
    "        \n",
    "        '''\n",
    "        OUTPUT: links.collect() will give a <K, Iterables<V> pairs)\n",
    "        [('article1', <pyspark.resultiterable.ResultIterable at 0x1c6e180d4c8>),\n",
    "         ('article2', <pyspark.resultiterable.ResultIterable at 0x1c6e180da48>),\n",
    "         ('article3', <pyspark.resultiterable.ResultIterable at 0x1c6e180d488>),\n",
    "         ('article4', <pyspark.resultiterable.ResultIterable at 0x1c6e180dd08>),\n",
    "         ('article5', <pyspark.resultiterable.ResultIterable at 0x1c6e180d0c8>)]\n",
    " \n",
    "        links.mapValues(list).collect() will give the values of the key instead of Iterables\n",
    "        [('article1', ['article2', 'article4']),\n",
    "         ('article2', ['article3']),\n",
    "         ('article3', ['article1']),\n",
    "         ('article4', ['article2']),\n",
    "         ('article5', ['article6'])]\n",
    "        '''\n",
    "        \n",
    "        ranks = adj_list.map(lambda key: (key[0], self.init_rank))\n",
    "        '''\n",
    "        Initializes the ranks to 1.\n",
    "        Output: initial_ranks.collect()\n",
    "        [('article1', 1),\n",
    "         ('article2', 1),\n",
    "         ('article3', 1),\n",
    "         ('article4', 1),\n",
    "         ('article5', 1)]\n",
    "        '''\n",
    "        for iteration in range(self.max_iter):\n",
    "            mapped_adj_list = self.mapper(adj_list,ranks)\n",
    "            '''\n",
    "            OUTPUT: mapped_adj_list.collect()\n",
    "            [('article2', 0.5), ('article4', 0.5), ('article3', 1.0), ('article1', 1.0), ('article2', 1.0), ('article6', 1.0)]\n",
    "            '''\n",
    "            ranks = self.reducer(mapped_adj_list)\n",
    "            '''\n",
    "            OUTPUT: ranks.collect()\n",
    "            [('article2', 1.4249999999999998), ('article3', 1.0), ('article1', 1.0), ('article6', 1.0), ('article4', 0.575)]\n",
    "            '''\n",
    "        \n",
    "        schema = StructType([StructField(str(i), StringType(), True) for i in range(2)])\n",
    "        df = spark.createDataFrame(ranks, schema)\n",
    "        top_df = df.orderBy(list(df.columns))\n",
    "        top_df.limit(5).repartition(1).write.csv(\"gs://testing-pyspark-123/PageRankOuptut/p1t3.csv\", sep='\\t')\n",
    "        \n",
    "        spark.stop()\n",
    "    \n",
    "    def mapper(self, adj_list, ranks):\n",
    "        # Generates Key-Value pair where Key is the node and the value is the page rank value of its incoming node\n",
    "        return adj_list.join(ranks).flatMap(lambda rank: self.getRank(rank[1][0], rank[1][1]))\n",
    "        \n",
    "    def reducer(self,mapped_adj_list):\n",
    "        # Aggregates the output from the mapper along with dampning effect\n",
    "        return mapped_adj_list.reduceByKey(add).mapValues(lambda rank: rank * 0.85 + 0.15)    \n",
    "\n",
    "    def create_adj_list(self,link):\n",
    "        link = link.encode('utf-8')\n",
    "        k,v = re.split(r'\\t+', link)\n",
    "        return k,v\n",
    "    \n",
    "    def getRank(self,links, rank):\n",
    "        num_link = len(links)\n",
    "        for link in links:\n",
    "            yield (link, rank / num_link)\n",
    "\n",
    "    def read(self):\n",
    "        return spark.read.text(self.path).rdd.map(lambda row: row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg = PageRank('gs://cs_4121/test_set/total_csv_q1/part-00000-930a10b9-a12d-4526-b873-2dd4624714ad-c000.csv',max_iter=10)\n",
    "pg.fit()\n",
    "\n",
    "#actual data: gs://cs_4121/test_set/total_csv_q6/part-00000-5aa8823e-4732-434c-824d-5801bf7f9de4-c000.csv\n",
    "#test data: gs://cs_4121/test_set/total_csv2/part-00000-720fa84e-bf12-46ca-89f9-b732a0b84b22-c000.csv\n",
    "#small wiki: gs://cs_4121/test_set/total_csv_q1/part-00000-930a10b9-a12d-4526-b873-2dd4624714ad-c000.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
